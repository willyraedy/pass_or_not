# Metis Project 2 - Will a bill pass the Illinois Legislature?

## Objective and Audience
The contrived audience for this project was a group of progressive policy organizations in Illinois. As such, they would be curious about how they could increase their chances of getting a bill passed. This places a high priority on interpretability. In addition, they would want to know when they have done enough to get a particular piece of legislation so they could allocate scarce resources elsewhere. As a result, it is more important to limit false positives than false negatives and I optimized for precision.

## Data
Legiscan has csv's of all the bills from the most recent Illinois congressional sessions. I then merged that with a [dataset](https://americanlegislatures.com/) on individual politician ideology estimates using a simple name matching function. I then scraped [ballotpedia](https://ballotpedia.org/Main_Page) for additional information about Illinois state senators, such as number of terms served, total fundraising, and whether they chair a committee. Finally, I scraped the [Illinois General Assembly website](http://www.ilga.gov/) to collect [witness slip data](http://www.senatorsteans.com/constituent-resources/how-to-submit-a-witness-slip).

## Scope
This was a two week, independent project using classification methods. Given these constraints, I looked for a single step in a bill's lifetime that had reasonably balanced classes while being meaningful. The largest number of senate bills fail because they never get a vote (called a third reading) and about half of the bills that get a second reading get a vote. So I limited the model to predicting at the time of the second reading whether a bill introduced in the senate will get a third reading.

## Initial Splits
After some initial EDA and basic modeling, I observed that my decision trees were consistently separating the bills introduced by three senators. These senators introduced drastically more bills and got very few of them passed. I chose to make an initial split separating those outlier senators. In addition, bills introduced by Republican vs. Democratic senators had differing relationships with a number of my features. Therefore, I also split on the party of the bill's author and focused my efforts on modeling bills introduced by Democratic senators. After these splits, the data became more imbalance with 80% of the remaining bills getting a third reading.

## Model Selection
The final model is soft voting classifier trained on randomly oversampled data. Three simple models were fitted on different domains. The author model is a logistic regression on a limited number of author features. The legislative support model is a logistic regression with total co-sponsors and whether or not the bill has bipartisan support. Finally, the hearing or community support model is a random forest on the number of witness slips and the percent that supported the bill. These predictions were then passed to a soft voting classifier with the hearing and author predictions weighted twice as heavily as the legislative support prediction. This stacked classifier outperformed any individual algorithm with the same collection of features. Finally, the threshold was set at 0.55 to optimize for precision.

## Interpretation
With a precision of 92% and a recall of 48%, the model can reliably identify the bills most likely to pass. However, it does not have much predictive value for the other bills. In addition, many of the coefficients have the opposite signs from what would be expected. For example, the coefficients on co-sponsors and author term are both negative and increasing the number of witness slips almost exclusively lowers the odds of a bill passing with an especially large decrease around 5 slips. This would imply more community interest and more legislative support decrease the chances that a bill passes and that novice senators are better at getting bills passed.

A more plausible explanation is that the model simply finds the most trivial bills. One of the major gaps in the data is that there was no readily available information capturing the scope, cost, or ambition of a particular bill. As a result, the model treats every bill the same. Assuming trivial bills are passed at a higher rate, if first term senators tend to put forward trivial bills that no one bothers to co-sponsor or submit witness slips for, then that would explain the observed coefficients.

To test this theory, I compared the True Postives with the False Negatives. Essentially, I wanted to ask the question, is there a systemic difference between the bills the model predicted correctly and the bills the model missed? Since the bills in both the True Positive and False Negative groups got a floor vote, I compared the number of nay votes as a proxy for the ambition of the bill. This couldn't be included in the model for obvious data leakage reasons. True positives had an average of 4.6 nay votes with a standard deviation of 10. False negatives had an average of 8 nay votes with a standard deviation of 15.

This supports the suspicion that the model is identifying more trivial bills rather than uncovering causal relationships. As a result, this model can provide little value in informing the strategies of organizations and future efforts should focus on collecting data capturing important characteristics of the bills themselves. This would allow the model to compare apples to apples and isolate the effects of the author and other characteristics the policy organizations could effect.
