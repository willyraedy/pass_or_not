{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB, MultinomialNB\n",
    "from sklearn.model_selection import train_test_split, cross_validate, GridSearchCV\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src')\n",
    "from data import fetch_model_data\n",
    "from model import evaluation\n",
    "%aimport data.fetch_model_data\n",
    "%aimport model.evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics=['roc_auc', 'accuracy', 'precision']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ip_address = '18.218.116.177'\n",
    "raw = fetch_model_data.fetch_model_data(ip_address)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_feature_set = [\n",
    "    'author_ideology',\n",
    "    'author_party',\n",
    "    'author_is_chair',\n",
    "    'author_years_sen',\n",
    "    'author_total_funding',\n",
    "    'total_sponsors',\n",
    "    'sponsor_chairs',\n",
    "    'agg_funding_sponsors',\n",
    "    'agg_exp_sponsors',\n",
    "    'total_slips',\n",
    "    'slips_perc_pro',\n",
    "    'bipartisan',\n",
    "    'ideol_range',\n",
    "    'first_word_approp',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = raw[first_feature_set], raw.third_reading\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=99, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>metric</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>in_sample</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>roc_auc</td>\n",
       "      <td>0.796013</td>\n",
       "      <td>0.028032</td>\n",
       "      <td>0.799573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>accuracy</td>\n",
       "      <td>0.803282</td>\n",
       "      <td>0.016401</td>\n",
       "      <td>0.807401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>precision</td>\n",
       "      <td>0.848561</td>\n",
       "      <td>0.004257</td>\n",
       "      <td>0.852670</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      metric      mean       std  in_sample\n",
       "0    roc_auc  0.796013  0.028032   0.799573\n",
       "1   accuracy  0.803282  0.016401   0.807401\n",
       "2  precision  0.848561  0.004257   0.852670"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = cross_validate(GaussianNB(), X_train, y_train, return_train_score=True, scoring=['roc_auc', 'accuracy', 'precision'], cv=5)\n",
    "evaluation.report_single_model_metrics(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bernoulli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>metric</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>in_sample</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>roc_auc</td>\n",
       "      <td>0.833535</td>\n",
       "      <td>0.010121</td>\n",
       "      <td>0.833343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>accuracy</td>\n",
       "      <td>0.808991</td>\n",
       "      <td>0.014163</td>\n",
       "      <td>0.812052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>precision</td>\n",
       "      <td>0.825905</td>\n",
       "      <td>0.012571</td>\n",
       "      <td>0.826501</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      metric      mean       std  in_sample\n",
       "0    roc_auc  0.833535  0.010121   0.833343\n",
       "1   accuracy  0.808991  0.014163   0.812052\n",
       "2  precision  0.825905  0.012571   0.826501"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = cross_validate(BernoulliNB(), X_train, y_train, return_train_score=True, scoring=['roc_auc', 'accuracy', 'precision'], cv=5)\n",
    "evaluation.report_single_model_metrics(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Higher ROC but worse precision\n",
    "    # Might be able to tune with threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multinomial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_validate(MultinomialNB(), X_train, y_train, return_train_score=True, scoring=['roc_auc', 'accuracy', 'precision'], cv=5)\n",
    "evaluation.report_single_model_metrics(scores)\n",
    "# Errors b/c of negative values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_features = [\n",
    "    'author_party',\n",
    "    'author_is_chair',\n",
    "    'bipartisan',\n",
    "    'first_word_approp',\n",
    "]\n",
    "\n",
    "count_features = [\n",
    "    'author_years_sen',\n",
    "    'total_sponsors',\n",
    "    'sponsor_chairs',\n",
    "    'agg_exp_sponsors',\n",
    "    'total_slips',\n",
    "]\n",
    "\n",
    "cont_features = [\n",
    "    'author_ideology',\n",
    "    'author_total_funding',\n",
    "    'agg_funding_sponsors',\n",
    "    'slips_perc_pro',\n",
    "    'ideol_range',\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Individually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>metric</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>in_sample</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>roc_auc</td>\n",
       "      <td>0.672511</td>\n",
       "      <td>0.014310</td>\n",
       "      <td>0.674444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>accuracy</td>\n",
       "      <td>0.667358</td>\n",
       "      <td>0.018401</td>\n",
       "      <td>0.667380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>precision</td>\n",
       "      <td>0.807921</td>\n",
       "      <td>0.016595</td>\n",
       "      <td>0.808272</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      metric      mean       std  in_sample\n",
       "0    roc_auc  0.672511  0.014310   0.674444\n",
       "1   accuracy  0.667358  0.018401   0.667380\n",
       "2  precision  0.807921  0.016595   0.808272"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = cross_validate(BernoulliNB(), X_train[cat_features], y_train, return_train_score=True, scoring=['roc_auc', 'accuracy', 'precision'], cv=5)\n",
    "evaluation.report_single_model_metrics(scores)\n",
    "# Just categoricals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# strange that this did so much worse\n",
    "# I guess BernoulliNB can get meaningful info from continuous/count data\n",
    "    # also might be that many of the features have lots of observations at 0\n",
    "    # So the count data behaves like categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>metric</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>in_sample</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>roc_auc</td>\n",
       "      <td>0.800460</td>\n",
       "      <td>0.033529</td>\n",
       "      <td>0.800338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>accuracy</td>\n",
       "      <td>0.814711</td>\n",
       "      <td>0.018691</td>\n",
       "      <td>0.812053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>precision</td>\n",
       "      <td>0.854451</td>\n",
       "      <td>0.009675</td>\n",
       "      <td>0.852317</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      metric      mean       std  in_sample\n",
       "0    roc_auc  0.800460  0.033529   0.800338\n",
       "1   accuracy  0.814711  0.018691   0.812053\n",
       "2  precision  0.854451  0.009675   0.852317"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = cross_validate(GaussianNB(), X_train[cont_features], y_train, return_train_score=True, scoring=['roc_auc', 'accuracy', 'precision'], cv=5)\n",
    "evaluation.report_single_model_metrics(scores)\n",
    "# Just continuous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>metric</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>in_sample</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>roc_auc</td>\n",
       "      <td>0.805835</td>\n",
       "      <td>0.019210</td>\n",
       "      <td>0.805682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>accuracy</td>\n",
       "      <td>0.656636</td>\n",
       "      <td>0.011113</td>\n",
       "      <td>0.652360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>precision</td>\n",
       "      <td>0.810848</td>\n",
       "      <td>0.014798</td>\n",
       "      <td>0.804794</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      metric      mean       std  in_sample\n",
       "0    roc_auc  0.805835  0.019210   0.805682\n",
       "1   accuracy  0.656636  0.011113   0.652360\n",
       "2  precision  0.810848  0.014798   0.804794"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = cross_validate(MultinomialNB(), X_train[count_features], y_train, return_train_score=True, scoring=['roc_auc', 'accuracy', 'precision'], cv=5)\n",
    "evaluation.report_single_model_metrics(scores)\n",
    "# Just count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "bern_pipe = Pipeline([\n",
    "    ('categorical_features', ColumnTransformer([('feature_subset', 'passthrough', cat_features)])),\n",
    "    ('bern', BernoulliNB())\n",
    "])\n",
    "multi_pipe = Pipeline([\n",
    "    ('count_features', ColumnTransformer([('feature_subset', 'passthrough', count_features)])),\n",
    "    ('bern', MultinomialNB())\n",
    "])\n",
    "gauss_pipe = Pipeline([\n",
    "    ('cont_features', ColumnTransformer([('feature_subset', 'passthrough', cont_features)])),\n",
    "    ('bern', BernoulliNB())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>metric</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>in_sample</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>roc_auc</td>\n",
       "      <td>0.835331</td>\n",
       "      <td>0.011689</td>\n",
       "      <td>0.835350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>accuracy</td>\n",
       "      <td>0.708134</td>\n",
       "      <td>0.015508</td>\n",
       "      <td>0.706902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>precision</td>\n",
       "      <td>0.802021</td>\n",
       "      <td>0.010350</td>\n",
       "      <td>0.800004</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      metric      mean       std  in_sample\n",
       "0    roc_auc  0.835331  0.011689   0.835350\n",
       "1   accuracy  0.708134  0.015508   0.706902\n",
       "2  precision  0.802021  0.010350   0.800004"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vc = VotingClassifier(\n",
    "    estimators=[('bern', bern_pipe), ('multi', multi_pipe), ('gauss', gauss_pipe)],\n",
    "    voting='soft'\n",
    ")\n",
    "scores = cross_validate(vc, X_train, y_train, return_train_score=True, scoring=['roc_auc', 'accuracy', 'precision'], cv=5)\n",
    "evaluation.report_single_model_metrics(scores)\n",
    "# Soft voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>metric</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>in_sample</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>accuracy</td>\n",
       "      <td>0.712424</td>\n",
       "      <td>0.015022</td>\n",
       "      <td>0.709047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>precision</td>\n",
       "      <td>0.806737</td>\n",
       "      <td>0.010437</td>\n",
       "      <td>0.802749</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      metric      mean       std  in_sample\n",
       "0   accuracy  0.712424  0.015022   0.709047\n",
       "1  precision  0.806737  0.010437   0.802749"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vc = VotingClassifier(\n",
    "    estimators=[('bern', bern_pipe), ('multi', multi_pipe), ('gauss', gauss_pipe)],\n",
    "    voting='hard'\n",
    ")\n",
    "scores = cross_validate(vc, X_train, y_train, return_train_score=True, scoring=['accuracy', 'precision'], cv=5)\n",
    "evaluation.report_single_model_metrics(scores, metrics=['accuracy', 'precision'])\n",
    "# Hard voting -> looks a little better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gaussian was the best one individually so let's weight that one higher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>metric</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>in_sample</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>accuracy</td>\n",
       "      <td>0.806129</td>\n",
       "      <td>0.015025</td>\n",
       "      <td>0.806150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>precision</td>\n",
       "      <td>0.825847</td>\n",
       "      <td>0.014117</td>\n",
       "      <td>0.825339</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      metric      mean       std  in_sample\n",
       "0   accuracy  0.806129  0.015025   0.806150\n",
       "1  precision  0.825847  0.014117   0.825339"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vc = VotingClassifier(\n",
    "    estimators=[('bern', bern_pipe), ('multi', multi_pipe), ('gauss', gauss_pipe)],\n",
    "    voting='hard',\n",
    "    weights=[1, 1, 5]\n",
    ")\n",
    "scores = cross_validate(vc, X_train, y_train, return_train_score=True, scoring=['accuracy', 'precision'], cv=5)\n",
    "evaluation.report_single_model_metrics(scores, metrics=['accuracy', 'precision'])\n",
    "# Hard voting -> looks a little better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seems like it's just approaching the Gaussian by itself\n",
    "    # Other things aren't adding much"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Takeaways**\n",
    "\n",
    "Seems like Bernoulli and Multinomial aren't very good by themselves. Could try stacking but I doubt it will help.\n",
    "\n",
    "Gaussian individually with just continuous features did just as good as when it had all of them.\n",
    "\n",
    "Bernoulli did much better with all the data. Seems like it makes everything <=zero a 0 and everything else a 1 by default.\n",
    "\n",
    "TODO: try mixing Guassian on just continuous, Bernoulii on everything. If promising, could coerce features to binary manually according to some unique threshhold for each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>metric</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>in_sample</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>roc_auc</td>\n",
       "      <td>0.831200</td>\n",
       "      <td>0.010319</td>\n",
       "      <td>0.830936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>accuracy</td>\n",
       "      <td>0.806126</td>\n",
       "      <td>0.016046</td>\n",
       "      <td>0.807760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>precision</td>\n",
       "      <td>0.824793</td>\n",
       "      <td>0.013367</td>\n",
       "      <td>0.825192</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      metric      mean       std  in_sample\n",
       "0    roc_auc  0.831200  0.010319   0.830936\n",
       "1   accuracy  0.806126  0.016046   0.807760\n",
       "2  precision  0.824793  0.013367   0.825192"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vc = VotingClassifier(\n",
    "    estimators=[('bern', BernoulliNB()), ('gauss', gauss_pipe)],\n",
    "    voting='soft',\n",
    ")\n",
    "scores = cross_validate(vc, X_train, y_train, return_train_score=True, scoring=['roc_auc', 'accuracy', 'precision'], cv=5)\n",
    "evaluation.report_single_model_metrics(scores)\n",
    "# Hard voting -> looks a little better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Still didn't do meaningfully better than just straight Bernoulli"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion: Bernoulli on everything is best"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:metis] *",
   "language": "python",
   "name": "conda-env-metis-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
